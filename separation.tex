%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    INSTITUTE OF PHYSICS PUBLISHING                                   %
%                                                                      %
%   `Preparing an article for publication in an Institute of Physics   %
%    Publishing journal using LaTeX'                                   %
%                                                                      %
%    LaTeX source code `ioplau2e.tex' used to generate `author         %
%    guidelines', the documentation explaining and demonstrating use   %
%    of the Institute of Physics Publishing LaTeX preprint files       %
%    `iopart.cls, iopart12.clo and iopart10.clo'.                      %
%                                                                      %
%    `ioplau2e.tex' itself uses LaTeX with `iopart.cls'                %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% First we have a character check
%
% ! exclamation mark    " double quote  
% # hash                ` opening quote (grave)
% & ampersand           ' closing quote (acute)
% $ dollar              % percent       
% ( open parenthesis    ) close paren.  
% - hyphen              = equals sign
% | vertical bar        ~ tilde         
% @ at sign             _ underscore
% { open curly brace    } close curly   
% [ open square         ] close square bracket
% + plus sign           ; semi-colon    
% * asterisk            : colon
% < open angle bracket  > close angle   
% , comma               . full stop
% ? question mark       / forward slash 
% \ backslash           ^ circumflex
%
% ABCDEFGHIJKLMNOPQRSTUVWXYZ 
% abcdefghijklmnopqrstuvwxyz 
% 1234567890
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\documentclass[12pt]{iopart}
\newcommand{\gguide}{{\it Preparing graphics for IOP Publishing journals}}
%Uncomment next line if AMS fonts required
\usepackage{iopams}  
\usepackage{graphicx}
%\usepackage{subcaption}

%\usepackage{amsmath,amssymb}
\begin{document}

\title[]{Leveraging local continuity of connected measurements for dimensionality reduction and separation of backround, signal, and noise.... }

\author{}

%%\address{IOP Publishing, Temple Circus, Temple Way, Bristol BS1 6HG, UK}
%\ead{submissions@iop.org}
%\vspace{10pt}
%\begin{indented}
%%\item[]August 2017
%\end{indented}

%\begin{abstract}
%This document describes the  preparation of an article using \LaTeXe\ and 
%\verb"iopart.cls" (the IOP Publishing \LaTeXe\ preprint class file).
%This class file is designed to help 
%authors produce preprints in a form suitable for submission to any of the
%journals listed in table~\ref{jlab1} on the next page.  You are not obliged to use this class file---we accept
%submissions using all common \LaTeX\ class and style files.  The \verb"iopart.cls"
%class file is supplied merely as a convenience for those authors who find it useful.
%This document gives both general advice that applies whatever class file you use, and specific advice
%that applies if you choose to use \verb"iopart.cls".
%
%We also accept submissions in Word format.  See elsewhere on this site for guidelines on Word submissions.
%
%If you have any queries about this document or any aspect of preparing your article for submission please contact us at the e-mail address given above.
%\end{abstract}

%
% Uncomment for keywords
%\vspace{2pc}
%\noindent{\it Keywords}: XXXXXX, YYYYYYYY, ZZZZZZZZZ
%
% Uncomment for Submitted to journal title message
%\submitto{\JPA}
%
% Uncomment if a separate title page is required
%\maketitle
% 
% For two-column output uncomment the next line and choose [10pt] rather than [12pt] in the \documentclass declaration
%\ioptwocol
%



\section{Introduction}
Experiments undertaken at synchrotron and XFEL light sources can generate large volumes of data that are rich in information but plagued by noise and artifacts. There is often an intrinsic connectedness between successive measurements taken on a progression of different physical states: for example, a series of thermodynamic states in a phase diagram. There may also be further data redundancy, as many methods--such as powder diffraction and serial crystallography--require repeated measurements of the same system to accumulate sufficient statistics or orientational averaging. Consequently, large spectroscopic or diffraction datasets generated at third- or fourth-generation light sources are likely to contain sequences of measurements that combine smooth variation with respect to underlying physical parameters with stochastic variation due to counting statistics, detector characteristics, sample nonuniformity and other artefacts. 
 %Define ordered and connected
These underlying physical parameters can be treated as ordinal variables to assemble a spectroscopic or (1D) diffraction dataset into a tensor of the form 

\begin{equation}
X_{i_1i_2...i_N, iq}
\end{equation}

where the first $i1$ to $i_N$ index the ordinal variables and $iq$ indexes energy or momentum transfer in the case of XRD or spectroscopy,
respectively. 
%for which each distinct sequence of values for the indices $i_1, i_2,...
%i_N$ corresponds to a single spectrum or diffraction pattern, $q$
%indexes momentum transfer or energy in the case of XRD or spectroscopy,
%respectively, and each of the indices $i_n, n \in \{1, 2, ... N\}$
%corresponds to an ordering dimension along which direction the
%underlying signal's variation is smooth. The dimensionality of the
%dataset is thus simply one higher than $N$, the number of ordinal
%attributes.
%

The connectedness of $X$ with respect to indices $i_1, i_2, ... i_N$ describes both the sample-derived signal and any smoothly-varying
background, while other contributions to the measured X-ray
intensity--such as shot noise, one-off artifacts, and stochastic
variations across different portions of the sample--are independent or
at least substantially stochastic. We label the signal and stochastic components of $X$ as $S$ and $\eta$, respectively: $X = S + \eta$.

\emph{perhaps notation shouldn't already be introduced in the intro section}
%(note: high-frequency variation is a
%more accurate description here, uncorrelated is a limiting case but it
%simplifies the interpretation).

In this paper, we take two examples of XRD datasets--one temperature
study of xxxx material and one combinatorial study of the CoNiTi
ternary system--to demonstrate the separation of connected data into
a physically relevant component and a component that comprises
noise and artifacts. We represent this as sepration of the signal into two components: $X = \hat{X} + \eta$, where
$\hat{X}$ is slow-varying in the ordinal dimensions and $\eta$ is a fast-varying noise
contribution (again, in the ordinal dimensions). The approach depends only on the minimal assumption of
connectedness and so should be applicable, with minor tuning, to any
dataset.

In the particular case of an XRD dataset the signal is concentrated in peaks that are narrow in $q$-space, which we show allows a robust separation of any background with slow $q$-variation (e.g. diffuse scattering). 

Finally, the most compelling exploitation of connected data emerges
in its \emph{interpretation}--especially in the propagation of noise
estimates into uncertainties in any downstream model-derived quantities,
and in the use of connectivity to remove domain-specific measurement
artifacts. In section xxxx we show a feature extraction scheme for XRD
phase mapping datasets that is based on local connectivity. The approach
is robust to peak-shifting, which until now has been long-standing
frustration for the interpretation of such datasets.
\emph{say why this is new and good, motivate and compare to prior efforts in the community}


%use  connectedness of measurements to separate a collection of spectral or diffraction
%
%In this paper we consider ways in which the redundancy of datasets connected by 
%
%This redundancy, we argue, creates an opportunity to use the mutual information accross different measurements to make.... information... robust or whaterver.... 
%
%Rich in information, plagued by noise and artifacts. 
%
%IN the paper, we want to show how we can leverage the connectedness of the measurements ... yada yadaa minimal assumptions, separate useful information from these large datasets.
%
%1. LS collect connected datasets
%2. rich in information but also artifacts and noise (background, peak shifting artifact, noise)
%    2. a Introduce Yijin and Suchi's data
%3. leveraging connectedness, we can separate these datasets. -> expand into the body of the paper.
%    3. a Define connectedness, state assumptions. Good measurements oversample the measrement space. We can use this oversampling to help separate datasets into different components with minimal assumptions. Connect 
%(work on the title)

%## Introduction
%* aim for JSR? large facilities (synchrotron, XFEL) produce large connected datasets (cites). 
%* Neighborhood and continuity can be exploited in two ways: to estimate noise (in the absence of a noise model) and to extract features. 
%* Motivating applications: combi; any 'dense' dataset over a range of values for the key experimental parameter.
%* Motivating problem: peak shifting is difficult to model; naive similarity metrics are not robust to it. What's the most parsimonious way of grouping XRD patterns for structures that are the same up to lattice parameter distortions? We will show that continuity + neighborhood is sufficient.
%* Discuss, cite prior literature

\section{Methods}

\subsection{Prior approaches}
Summarize approaches using simple similarity measures, such as cosine distance in the diffraction space.

\begin{figure}
  \includegraphics[width=\linewidth]{paper_figures/1/raw_with_similarity.png}
  \caption{}
  \label{fig:noise}
\end{figure}

\begin{figure}
  %\includegraphics[width=\linewidth]{figures/heatmaps.png}
  \includegraphics[scale=.5]{paper_figures/pkg/2_T_summary.png}
  \caption{ (a) Heatmap of XRD dataset $X_{iq}$ corresponding to a temperature scan
of XRD patterns for xxxxx system. The vertical index $i$ and horizontal
index $q$ index temperature and momentum transfer, respectively. }
  \label{fig:heatmaps}
\end{figure}
%Separation: either fast/slow or frequency-based variation. Frequency of variation is the key idea. 
%
%Alternative: median / quartile variation based on population density. Acknowledge these alternatives. 
%
%Consider cost of computation: what's the method that would allow an event-detection-type setup. See if we could sell this type of thing to the detector group at SLAC that works on ASICS for rare evetns. 
\subsection{Background subtraction}
The first step to estimating the background is to identify the peak
regions, which must be excluded before calculating the background. In
the case of typical diffraction data a simple high-pass DFT filter does
not cleanly extract the peaks when phase is retained in the inverse
Fourier transform. This is because of the presence of ringing artifacts
as well as the high concentration of signal in the diffraction peaks,
which pile on top of the background in the low-frequency region of the
power spectral density. On the other hand we can identify peak regions
by padding the signal in the $q$ dimension, taking a 1D DFT, and
suppressing low-frequency components with a simple filter:

% TODO effect of zero-padding / extrapolation
\begin{equation}
N(0, \sigma) \circledast |{F}^{-1}(H  {F}(q))|,
\end{equation}

where $N$ is a unit Gaussian in q with standard deviation $\sigma$
is chosen to match the diffraction peak width, $\circledast$ denotes
convolution, and $H$ is a Blackman window 

\emph{this part will be fleshed out. some steps are still omitted}

The background is then estimated by an interpolation using data from non-peak regions to
fill in background intensities within the peak regions (Fig. 1(c)).
Finally, it is subtracted from the denoised data (Fig. 1(d)).

To make for a simple extension to datasets of arbitrary dimensions we
estimate the background by linear interpolation in the q dimension
alone, together with an N-dimensional nearest-neighbor assignment to
fill in points out of range for interpolation.

\subsection{Noise estimation}
\begin{figure}
  \includegraphics[width=\linewidth]{figures/noise.png}
  \caption{}
  \label{fig:noise}
\end{figure}
While in the case of intensity variation in the q dimension we assume
that high-frequency features belong to the informative (i.e. physical)
component of the signal, in the case of the ordering dimensions we
make the opposite assumption that the physically relevant part of the
signal varies smoothly from one XRD pattern to any adjacent one, while
any high-frequency, uncorrelated deviations from this progression are
due to either noise (from e.g. detector characteristics or Poisson
statistics) or artifacts (such as insufficient orientational sampling
of the diffracting crystallites). Under this assumption the signal and
noise lack substantial overlap in the N-1-dimensional Fourier transform
along ordering dimensions; thus we can use a simple DFT filter to
separate them (Fig. 2)

The above estimations of background and noise components separate
the signal into estimates of the signal component $S$ (which in turn separates into
background + diffuse scattering and diffraction) and uncertainty values
that are interpreted as single samples of an unknown underlying noise distributions. Despite its poorly-characterized nature, the latter of these can be used to propagate uncertainties to any downstream analyses. 

We label the signal and background $\hat{S}$ and $B$, respectively,
so that $X = \hat{S} + B + \eta$.



\emph{some discussion is needed on the difference in interpretation of the noise estimate in undersampled vs. sufficiently sampled datasets}

\section{Methods and Discussion}
\subsection{Application: feature extraction}
The above analysis addresses the data itself, with no scientific
interpretation aside from the separation between crystalline and diffuse
contributions to the scattering signal. However, we find that the same
ordering and smoothness properties are useful for reducing the data into
salient \emph{information}, where the goal is to find physically-meaningful
boundaries in the ordering dimensions corresponding to, e.g., the
surface separating a single-phase region from surrounding multi-phase
regions in a combinatorial diffraction dataset. Specifically, we 
identify diffraction peaks in every XRD pattern, independently, and then
link each peak in a pattern to any q-adjacent peaks in neighboring patterns (i.e. patterns that are adjacent in $i_1, i_2 ... i_N$). 

Each contiguous set of peaks linked in this way is denoted as a feature, and the ensemble of features defines a new representation of the dataset $ Y_{i_1,i_2...i_N, j} $, where $j$ indexes the peak features. The entries of $Y$ are 0 where the corresponding pattern lacks feature $j$, and equal to the intensity of the feature-peak combination otherwise.

\emph{here, explain two ways of getting peak locations and intensity: the simple ridge-finding and BBA-based peak-fitting.}
Peak parameters are obtained using a curve-fitting procedure that relies on Scargle's Bayesian Block algorithm to segment each measurement and then performs iterative peak-fitting on every block through a nonlinear least squares optimization on a sum of peak profiles. A Voigt profile is used in this work as it is appropriate for powder diffraction data. 

Peak profiles are added to the fit curve until the fit residual satisfies

$
|R \oslash \eta| < s,
$

where s is a chosen threshold of order unity and $\oslash$ denotes elementwise division.

\emph{maybe talk about regularization here}

\emph{cover the utility of noise estimates and background subtraction for better peak fitting}

\begin{figure}
  \includegraphics[width=\linewidth]{paper_figures/pkg/3_BBA.png}
  \caption{Example of the effect of background subtraction and noise
 estimation on peak fitting for one BBA block. This figure needs to be
 reformatted (font size, layout)}
  \label{fig:features}
\end{figure}

%utility of this is that it removes peak-shifting,
%etc….

\begin{figure}
  \includegraphics[width=\linewidth]{paper_figures/pkg/4_Tfeatures.png}
  \caption{(a) Features identified from peaks connected in
$i,q$ space for XRD dataset $X_{iq}$ corresponding to a temperature scan
of XRD patterns for xxxxxx system. Index $i$ denotes temperature.
    
(b) Intensity profile along each of the features in (a).}
  \label{fig:features}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{paper_figures/pkg/5_T_peakfit.png}
\caption{}
  \label{fig:simsquares}
\end{figure}

\begin{figure}[h!]
  \centering
%  \begin{subfigure}[b]{0.4\linewidth}
%    \includegraphics[width=\linewidth]{figures/3d_features.png}
%    \caption{(a)}
%  \end{subfigure}
%  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{paper_figures/pkg/6.png}
%    \caption{(b)}
%  \end{subfigure}
  \caption{caption}
  \label{fig:6}
\end{figure}

\begin{figure}[h!]
  \centering
%  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[scale = .55]{paper_figures/pkg/7_composition_summary_peakfit.png}
%    \caption{(a)}
%  \end{subfigure}
%  \begin{subfigure}[b]{0.4\linewidth}
%    \caption{(b)}
%  \end{subfigure}
  \caption{caption}
  \label{fig:7}
\end{figure}

%* Discuss caveats: some features are broken up, others are merged. Since there's typically plenty of reduncancy due to the multiple lines that appear with each phase, we think that the feature extraction is robust to these errors.
%* Further interpretation: this is a dimensionality reduction technique. Not a scientific methodology in itself (i.e. the extracted information still has to be interpreted), but a possible improvment over prior methods that are very fragile with respect to peak shifts. 

\section{Conclusions and future work}

\section{References}

\end{document}
